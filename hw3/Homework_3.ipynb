{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1** Get the heart disease data from [this site](http://www.stat.cmu.edu/~larry/all-of-statistics/=data/coris.dat). Consider a Bayesian analysis of the logistic regression model (chapter 4.3.2 in [2]). \n",
    "\n",
    "$$P(Y=1 \\left |  \\right . X=x ) = \\frac{e^{\\beta_0 + \\boldsymbol{\\beta} x}}{1+e^{\\beta_0 + \\boldsymbol{\\beta} x}}$$\n",
    "$$ \\boldsymbol{\\beta} = (\\beta_1, \\beta_2, ... , \\beta_k)$$\n",
    "\n",
    "Use the flat prior $f(\\beta_0,...,\\beta_k) \\propto 1$. Use the Gibbs-Metropolis algorithm (Chapter 24 in [4]) to draw sample of size $10,000$ from the posterior $f(\\beta_0,\\beta_1 \\left |  \\right . data)$. Plot histograms of the posteriors for the $\\beta_j$'s. Get the posterior mean and a 95 percent posterior interval for each $\\beta_j$.\n",
    "\n",
    "Compare your analysis to a frequentist approach using maximum likelihood (Chapter 9.3 in [4]), you can use **PyMC3** module to perform sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2** Illustrate the mean-field approximation and its use in variational Bayesian Inference (chapter 10 in [2] and [3]) using a linear regression model, on [Boston house prices dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html).\n",
    "\n",
    "$$y_i = \\mathbf{x_i} \\boldsymbol{\\beta} + \\epsilon_i ,$$\n",
    "\n",
    "$$\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),  i=1...N , $$\n",
    "\n",
    "with semi-conjugate priors,\n",
    "\n",
    "$$\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\beta, \\mathbf{\\Sigma_\\beta})$$\n",
    "$$\\sigma^2 \\sim InverseGaussian(a, b)$$\n",
    "\n",
    "Find an approximation of the posterior distribution $q(\\boldsymbol{\\beta}, \\sigma^2)$ by replacing it with factorisation of simpler probability densities[1].\n",
    "\n",
    "\n",
    "Write implementation of the variational search and visualize convergence of ELBO (Empirical Lower Bound)[5] defined as: $$L(q) = E \\left|_q[\\log\\ q(\\boldsymbol{\\beta}, \\sigma^2)]\\right. − E\\left|_q[\\log (q^*(\\boldsymbol{\\beta})\\cdot q^*(\\sigma^2))]\\right.$$,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "1. Ormerod, J. T. and M. P. Wand (2010). “Explaining variational approximations,” The American Statistician,\n",
    "64(2), 140-153. [[pdf](http://ro.uow.edu.au/cgi/viewcontent.cgi?article=1026&context=cssmwp)]\n",
    "2. Bishop, C. M. (2006). Pattern recognition. Machine Learning.\n",
    "3. Ansari, A. (2014). Variational Bayesian Inference for Big Data Marketing Models (Doctoral dissertation, University of Washington). [[pdf](https://bus.wisc.edu/~/media/bus/knowledge-expertise/academic-departments/marketing/yang-li-paper.pdf?la=en)]\n",
    "4. Wasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media. [[site](http://www.stat.cmu.edu/~larry/all-of-statistics/)]\n",
    "5. https://hips.seas.harvard.edu/blog/2013/03/22/variational-inference-part-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
